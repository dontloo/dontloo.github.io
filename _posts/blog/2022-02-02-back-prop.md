---
layout: post
title: "Backpropagation"
modified:
categories: blog
excerpt:
tags: []
date: 2022-02-02
modified: 2022-02-02
---

### Backpropagation
At training time, a neural network is essentially a funtion that takes a vector and outputs a scalar value, \\(loss=l(f(\mathbf{x}))\\). 
Backpropagation is the process of computing the derivative by applying the chain rule,
\\[\frac{\partial loss}{\partial \mathbf{x}}=\frac{\partial loss}{\partial \mathbf{f}}\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\\]
where \\(\mathbf{f}\\) is the network predictions, which is usualy a vector.

### Jacobian matrix
The derivative of a scalar w.r.t a vector is a vector is of the same shape as the input.
The derivative of a vector w.r.t a vector is a vector is a matrix known as the Jacobian matrix.
So in the above formula, \\(\partial loss/\partial \mathbf{f}\\) is a vector, \\(\partial \mathbf{f}/\partial \mathbf{x}\\) is a matrix where \\((\partial \mathbf{f}/\partial \mathbf{x})_{ij}=\partial \mathbf{f}_i/\partial \mathbf{x}_j\\).


### Chain Rule for Multivariable Functions
The chain rule applied for multivariable functions is equal to the matrix product of Jacobians.
Say the computation dependency between three variables is \\(\mathbf{x} \rightarrow \mathbf{h} \rightarrow \mathbf{f}\\),
by the [sum rule of derivatives](https://en.wikipedia.org/wiki/Differentiation_rules#Differentiation_is_linear), a single element of the Jacobian matrix \\(\partial \mathbf{f}/\partial \mathbf{x}\\) is
\\[\frac{\partial \mathbf{f}}{\partial \mathbf{x}}_{ij}=\sum_k \frac{\partial \mathbf{f}_i}{\partial \mathbf{h}_k}\frac{\partial \mathbf{h}_k}{\partial \mathbf{h}_j}\\]
which is the inner product of the \\(i\\)th row of \\(\partial \mathbf{f}/\partial \mathbf{h}\\) and the \\(j\\)th column of \\(\partial \mathbf{h}/\partial \mathbf{x}\\), which is by definition the product of the two Jacobian matrices.


### VJP
https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#advanced-topic-more-autograd-detail-and-the-high-level-api

