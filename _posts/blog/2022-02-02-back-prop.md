---
layout: post
title: "Backpropagation"
modified:
categories: blog
excerpt:
tags: []
date: 2022-02-02
modified: 2022-02-02
---

### Backpropagation
At training time, a neural network is essentially a funtion that takes a vector and outputs a scalar value, \\(loss=l(f(\mathbf{x}))\\). 
Backpropagation is the process of computing the derivative by applying the chain rule,
\\[\frac{\partial loss}{\partial \mathbf{x}}=\frac{\partial loss}{\partial \mathbf{f}}\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\\]
where \\(\mathbf{f}\\) is the network predictions, which is usualy a vector.

### Jacobian matrix
The derivative of a scalar w.r.t a vector is a vector is of the same shape as the input.
The derivative of a vector w.r.t a vector is a vector is a matrix known as the Jacobian matrix.
So in the above formula, \\(\frac{\partial loss}{\partial \mathbf{f}}\\) is a vector, \\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\\) is a matrix.


### Chain Rule for Multivariable Functions
The chain rule for multivariable functions is actually the matrix product of Jacobians.
TODO: proof


### VJP
https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#advanced-topic-more-autograd-detail-and-the-high-level-api

